{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "crawlYahooFinance.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "BdOwvJngGoXs",
        "colab_type": "code",
        "outputId": "7e1e9eaf-054b-4f25-d12e-7c494e0025b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import bs4 as bs\n",
        "import urllib.request\n",
        "import pandas as pd\n",
        "from time import sleep\n",
        "from lxml import html  \n",
        "import requests\n",
        "import json\n",
        "import argparse\n",
        "from collections import OrderedDict\n",
        "import lxml.html as lh\n",
        "import copy\n",
        "import pandas as pd\n",
        "import urllib3\n",
        "!pip install --upgrade -q pygsheets\n",
        "!pip install --upgrade -q gspread\n",
        "from google.colab import drive\n",
        "drive.mount('drive')\n",
        "import gspread\n",
        "import pygsheets"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at drive; to attempt to forcibly remount, call drive.mount(\"drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LwCrnc2uiuYn",
        "colab_type": "code",
        "outputId": "0952a0f0-0df2-433f-d742-5c7b4c070008",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "auth.authenticate_user()\n",
        "gc = gspread.authorize(GoogleCredentials.get_application_default())\n",
        "worksheet = gc.open('TICS').sheet1\n",
        "\n",
        "# get_all_values gives a list of rows.\n",
        "rows = worksheet.get_all_values()\n",
        "\n",
        "# Convert to a DataFrame and render.\n",
        "import pandas as pd\n",
        "data = pd.DataFrame.from_records(rows, index = None)\n",
        "cmpList = data[data.columns[0]].astype(str).values.tolist()\n",
        "print('Number of tickets', len(cmpList))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of tickets 13\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rctUXUkK6uR0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sleep_val = 3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xw-PfKtGQHhs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Brute force way: not used\n",
        "def getSummaryData(url):\n",
        "  response = requests.get(url, verify=False)\n",
        "  print (\"Parsing %s\"%(url))\n",
        "  sleep(sleep_val)\n",
        "  parser = html.fromstring(response.text)\n",
        "  summary_table = parser.xpath('//div[contains(@data-test,\"summary-table\")]//tr')\n",
        "  summary_data = OrderedDict()\n",
        "  try:\n",
        "    for table_data in summary_table:\n",
        "      raw_table_key = table_data.xpath('.//td[contains(@class,\"C($primaryColor)\")]//text()')\n",
        "      raw_table_value = table_data.xpath('.//td[contains(@class,\"Ta(end)\")]//text()')\n",
        "      table_key = ''.join(raw_table_key).strip()\n",
        "      table_value = ''.join(raw_table_value).strip()\n",
        "      summary_data.update({table_key:table_value})\n",
        "    return summary_data\n",
        "  \n",
        "  except:\n",
        "    print (\"Failed to parse json response\")\n",
        "    return {\"error\":\"Failed to parse json response\"}\n",
        "  \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFI2zPu-QKRV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for parsing yahoo finance data\n",
        "#items = ['/financials?p=', '/balance-sheet?p=', '/cash-flow?p=']\n",
        "\n",
        "financialslen = 5\n",
        "balancesheetlen = 4\n",
        "cashflowlen = 5\n",
        "\n",
        "def getFinanceData(url, name):\n",
        "  global financialslen\n",
        "  global balancesheetlen\n",
        "  global cashflowlen\n",
        "  try:\n",
        "    #urllib.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
        "    response = requests.get(url, verify=False)\n",
        "    # response = http.request('GET', url)\n",
        "    sleep(sleep_val)\n",
        "    parser = html.fromstring(response.text)\n",
        "    finance_table = parser.xpath('//div[contains(@data-test,\"fin-row\")]')\n",
        "    \n",
        "    keys = []\n",
        "    values = []\n",
        "    \n",
        "    columnLen = 0 \n",
        "    for table_data in finance_table:\n",
        "      raw_table_key = table_data.xpath('.//span[contains(@class,\"Va(m)\")]//text()')\n",
        "      if len(raw_table_key) > 1:\n",
        "        continue\n",
        "        \n",
        "      raw_table_value = table_data.xpath('.//span[not(contains(@class,\"Va(m)\"))]//text()')\n",
        "      \n",
        "      # raw_table_key has only single element\n",
        "      keys.append(raw_table_key[0])\n",
        "      values.append(raw_table_value)\n",
        "      columnLen = max(columnLen, len(raw_table_value))\n",
        "    \n",
        "    # Solving mismatch in columns\n",
        "    if name == 'financials':\n",
        "      columnLen = financialslen\n",
        "    \n",
        "    elif name == 'balance-sheet':\n",
        "      columnLen = balancesheetlen\n",
        "    \n",
        "    elif name == 'cash-flow':\n",
        "      columnLen = cashflowlen\n",
        "        \n",
        "        \n",
        "    columns = []\n",
        "    data = []\n",
        "    \n",
        "    for row in keys:\n",
        "      for i in range(1, columnLen + 1):\n",
        "        columns.append(row + \"_\" + str(i))\n",
        "        \n",
        "    for value in values:\n",
        "      temp = []\n",
        "      for item in value:\n",
        "        temp.append(item)\n",
        "\n",
        "      if len(value) < columnLen:\n",
        "        diff = columnLen - len(value)\n",
        "        while diff > 0:\n",
        "          temp.append(\"N/A\")\n",
        "          diff = diff -1\n",
        "      \n",
        "      data.extend(temp)\n",
        "    \n",
        "    finance_data = pd.DataFrame()\n",
        "    \n",
        "    columns_series = pd.Series(columns, index  = None)\n",
        "    finance_data = pd.DataFrame([data], columns = columns)\n",
        "    \n",
        "    return finance_data\n",
        "  except:\n",
        "    return pd.DataFrame()\n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LpC5ggz-twKQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def parse_url(url, cmp):\n",
        "  try:\n",
        "    response = requests.get(url)\n",
        "    sleep(sleep_val)\n",
        "    soup = bs.BeautifulSoup(response.text, 'lxml')\n",
        "    \n",
        "    listdf = []\n",
        "    \n",
        "    for table in soup.find_all('table'):\n",
        "      listdf.append(parse_html_table(table, cmp))\n",
        "    \n",
        "    return listdf\n",
        "  except:\n",
        "    return []\n",
        "    \n",
        "def parse_html_table(table, cmp):\n",
        "  n_columns = 0\n",
        "  n_rows=0\n",
        "  column_names = []\n",
        "\n",
        "  # Find number of rows and columns\n",
        "  # we also find the column titles if we can\n",
        "  for row in table.find_all('tr'):\n",
        "\n",
        "      # Determine the number of rows in the table\n",
        "      td_tags = row.find_all('td')\n",
        "      if len(td_tags) > 0:\n",
        "          n_rows+=1\n",
        "          if n_columns == 0:\n",
        "              # Set the number of columns for our table\n",
        "              n_columns = len(td_tags)\n",
        "\n",
        "      # Handle column names if we find them\n",
        "      th_tags = row.find_all('th') \n",
        "      if len(th_tags) > 0 and len(column_names) == 0:\n",
        "          for th in th_tags:\n",
        "              column_names.append(th.get_text().replace(cmp, \"cmp\"))\n",
        "  \n",
        "  # Safeguard on Column Titles\n",
        "  if len(column_names) > 0 and len(column_names) != n_columns:\n",
        "      raise Exception(\"Column titles do not match the number of columns\")\n",
        "\n",
        "  columns = column_names if len(column_names) > 0 else range(0,n_columns)\n",
        "  df = pd.DataFrame(columns = columns,\n",
        "                    index= range(0,n_rows))\n",
        "  row_marker = 0\n",
        "  for row in table.find_all('tr'):\n",
        "      column_marker = 0\n",
        "      columns = row.find_all('td')\n",
        "      for column in columns:\n",
        "          df.iat[row_marker,column_marker] = column.get_text()\n",
        "          column_marker += 1\n",
        "      if len(columns) > 0:\n",
        "          row_marker += 1\n",
        "\n",
        "  # Convert to float if possible\n",
        "  for col in df:\n",
        "      try:\n",
        "          df[col] = df[col].astype(float)\n",
        "      except ValueError:\n",
        "          pass\n",
        "\n",
        "  return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tw_g6LL7CVQ9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Brute force way: not used\n",
        "def getProfileData(url):\n",
        "  try:\n",
        "    response = requests.get(url, verify=False)\n",
        "    print (\"Parsing %s\"%(url))\n",
        "    sleep(sleep_val)\n",
        "    \n",
        "    parser = html.fromstring(response.text)\n",
        "    profile_list = parser.xpath('//div[contains(@data-test,\"asset-profile\")]')\n",
        "    \n",
        "    columns = []\n",
        "    rows = []\n",
        "    \n",
        "    \n",
        "    if len(profile_list) > 0:\n",
        "      profile_table = profile_list[0] # take the first element (need not iterate)\n",
        "      address_list = profile_table.xpath('.//p[contains(@class,\"D(ib) W(47.727%) Pend(40px)\")]//text()')\n",
        "      address = \"\"\n",
        "      for a in address_list:\n",
        "        address = address + a + \" \" \n",
        "\n",
        "      columns.append(\"Address\")\n",
        "      rows.append(address)\n",
        "\n",
        "      info_list = profile_table.xpath('.//p[contains(@class,\"D(ib) Va(t)\")]//text()')\n",
        "\n",
        "      for i, val in enumerate(info_list):\n",
        "\n",
        "        if (i - 1)%3 == 0: # removing junk char\n",
        "          continue\n",
        "\n",
        "        if i%3 == 0:\n",
        "          columns.append(val)\n",
        "        else:\n",
        "          rows.append(val)\n",
        "    \n",
        "    description_list = parser.xpath('//section[contains(@class,\"quote-sub-section Mt(30px)\")]')\n",
        "    if len(description_list) > 0:  \n",
        "      description_table = description_list[0]\n",
        "      description = description_table.xpath('.//p[contains(@class,\"Mt(15px) Lh(1.6)\")]//text()')\n",
        "      columns.append(\"Description\")\n",
        "      rows.append(description[0])\n",
        "    \n",
        "    corpgov_list = parser.xpath('//section[contains(@class,\"Mt(30px) quote-section corporate-governance-container\")]')\n",
        "    if len(corpgov_list) > 0:\n",
        "      corpgov_table = corpgov_list[0]\n",
        "      corpgov = corpgov_table.xpath('.//p[contains(@class,\"Fz(s)\")]//text()')\n",
        "\n",
        "      columns.append(\"Corporate Goverance\")\n",
        "      rows.append(corpgov[0])\n",
        "\n",
        "    \n",
        "    if len(rows) != len(columns):\n",
        "      print('Error in profile non tabular data')\n",
        "      \n",
        "    finance_data = pd.DataFrame()\n",
        "    \n",
        "    columns_series = pd.Series(columns, index  = None)\n",
        "    profile_data = pd.DataFrame([rows], columns = columns)\n",
        "    \n",
        "    \n",
        "    return profile_data\n",
        "  \n",
        "  except:\n",
        "    return pd.DataFrame()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fPodJIKYG2UA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def writeYahooSummary():\n",
        "  # Parsing data from yahoo finance (profile, analysis, summary)\n",
        "  print(\"\\nWriting Yahoo Summary\")\n",
        "  towriteCmpList = []\n",
        "  ignored_cmp = []\n",
        "  #https://finance.yahoo.com/quote/%5EIXIC/components?p=%5EIXIC\n",
        "\n",
        "  items = ['']\n",
        "  urlPre = 'https://finance.yahoo.com/quote/'\n",
        "  urlSuf = '?p='\n",
        "\n",
        "  columns = None\n",
        "\n",
        "  final_df = pd.DataFrame()\n",
        "\n",
        "  for cmp in cmpList:\n",
        "    cmp_df_list = []\n",
        "    print('\\nCompany: ', cmp)\n",
        "    \n",
        "    for item in items:\n",
        "      url = urlPre + cmp + item + urlSuf + cmp\n",
        "      dataframe_list = parse_url(url, cmp)\n",
        "      \n",
        "      if item is '':\n",
        "        name = 'summary'\n",
        "      else:\n",
        "        name = item[1:len(item)]\n",
        "        \n",
        "      count = 1\n",
        "      \n",
        "      item_df = pd.DataFrame()\n",
        "      \n",
        "      \n",
        "      for df in dataframe_list:\n",
        "      \n",
        "        # Summary (2), Analysis (6), Profile (1)\n",
        "        \n",
        "        \n",
        "        if item is \"\":\n",
        "          df = df.transpose()\n",
        "          item_df = pd.concat([item_df, df], axis = 1, ignore_index = True)\n",
        "          \n",
        "          \n",
        "        if item == \"/analysis\": \n",
        "          cols_total = df.shape[1]\n",
        "          column_heads = []\n",
        "          for head in range(1, cols_total + 1):\n",
        "            column_heads.append(str(head))\n",
        "          \n",
        "          df.columns = column_heads\n",
        "          \n",
        "          df.set_index(df.columns[0], inplace=True)\n",
        "          df = df.replace({pd.np.nan: None})\n",
        "          df = df.unstack().to_frame().sort_index(level=1).T\n",
        "          \n",
        "          df.columns = df.columns.map(\"_\".join)\n",
        "          \n",
        "          item_df = pd.concat([item_df, df], axis = 1)\n",
        "          \n",
        "      \n",
        "        if item == \"/profile\":     \n",
        "          column_Names = df.columns\n",
        "          columns = []\n",
        "          rows = df.shape[0]\n",
        "\n",
        "          for j in range(1, rows + 1):\n",
        "            for i in column_Names:\n",
        "              columns.append(str(i) + \"_\" + str(j))\n",
        "\n",
        "          columns_series = pd.Series(columns, index  = None)\n",
        "          values_series = pd.Series(df.values.flatten(), index = None)\n",
        "          values_list = [df.values.flatten()]\n",
        "          item_df = pd.DataFrame(values_list, columns = columns)\n",
        "\n",
        "      if item is \"\":\n",
        "        new_header = item_df.iloc[0] #grab the first row for the header\n",
        "        item_df.columns = new_header #set the header row as the df header\n",
        "        item_df = item_df[1:] #take the data less the header row\n",
        "        item_df.reset_index(inplace=True, drop=True)\n",
        "      \n",
        "      \n",
        "      #print(name, len(dataframe_list), 'tables ', item_df.shape[1], 'Columns')\n",
        "      cmp_df_list.append(item_df)\n",
        "      \n",
        "  #     filename = cmp + '_' + name + '.csv'\n",
        "  #     item_df.to_csv(filename, index = False)\n",
        "  #     !cp $filename drive/My\\ Drive/Research/AB/\n",
        "    \n",
        "    \n",
        "    # Non tabular Profile \n",
        "    profile_url = urlPre + cmp + \"/profile\" + urlSuf + cmp\n",
        "    profile_data = getProfileData(profile_url)\n",
        "    cmp_df_list.append(profile_data)\n",
        "    \n",
        "    cmp_df = pd.DataFrame()\n",
        "    for df in cmp_df_list:\n",
        "      cmp_df = pd.concat([cmp_df,df], axis = 1)\n",
        "    \n",
        "    # Causes problem when company name is \"F\" and it gets replaced everywhere in the column names. Better to do in parse_url\n",
        "    # cmp_df.columns = cmp_df.columns.str.replace(cmp, \"cmp\")\n",
        "    \n",
        "    if columns == None:\n",
        "      columns = cmp_df.columns.tolist()\n",
        "  \n",
        "    #cmp_df.sort_index(axis=1, inplace=True) # not required if string replace done at parsing time\n",
        "    \n",
        "    if (final_df.shape[0] != 0 and final_df.columns.equals(cmp_df.columns) == False ):\n",
        "      # except when final_df is empty. This should return always True. Else column mistmatch (ignore this company)\n",
        "      print('Column mismatch for company: ', cmp, ' Ignoring...')\n",
        "      #print('differences: ', cmp_df.columns.difference(final_df.columns), final_df.columns.difference(cmp_df.columns)) #should always print 0\n",
        "      ignored_cmp.append(cmp)\n",
        "      continue\n",
        "      # MSFT: has current year (2020): typo\n",
        "      # ADBE: has different dates\n",
        "    else:\n",
        "      towriteCmpList.append(cmp)\n",
        "    \n",
        "    \n",
        "    \n",
        "    final_df = pd.concat([final_df, cmp_df], ignore_index = True)\n",
        "\n",
        "  final_df['company'] = towriteCmpList  \n",
        "  final_df = final_df.set_index('company')\n",
        "  print('Final Data: ',final_df.shape)\n",
        "\n",
        "  filename = 'YahooDataSummaryLarge.xlsx'\n",
        "  final_df.to_excel(filename)\n",
        "  !cp $filename drive/My\\ Drive/Research/AB/\n",
        "\n",
        "  if len(ignored_cmp) > 0:\n",
        "    ignored_csv = pd.DataFrame(ignored_cmp)\n",
        "    ignored_filename = 'YahooDataSummaryLarge_ignoredcmp.xlsx'\n",
        "    ignored_csv.to_excel(ignored_filename, index=\"False\")\n",
        "    !cp $ignored_filename drive/My\\ Drive/Research/AB/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HhKI-9FC1-p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def writeYahooProfile():\n",
        "  # Parsing data from yahoo finance (profile, analysis, summary)\n",
        "  print(\"\\nWriting Yahoo Profile\")\n",
        "  towriteCmpList = []\n",
        "  ignored_cmp = []\n",
        "  #https://finance.yahoo.com/quote/%5EIXIC/components?p=%5EIXIC\n",
        "\n",
        "  items = ['/profile']\n",
        "  urlPre = 'https://finance.yahoo.com/quote/'\n",
        "  urlSuf = '?p='\n",
        "\n",
        "  columns = None\n",
        "\n",
        "  final_df = pd.DataFrame()\n",
        "\n",
        "  for cmp in cmpList:\n",
        "    cmp_df_list = []\n",
        "    print('\\nCompany: ', cmp)\n",
        "    \n",
        "    for item in items:\n",
        "      url = urlPre + cmp + item + urlSuf + cmp\n",
        "      dataframe_list = parse_url(url, cmp)\n",
        "      \n",
        "      if item is '':\n",
        "        name = 'summary'\n",
        "      else:\n",
        "        name = item[1:len(item)]\n",
        "        \n",
        "      count = 1\n",
        "      \n",
        "      item_df = pd.DataFrame()\n",
        "      \n",
        "      \n",
        "      for df in dataframe_list:\n",
        "      \n",
        "        # Summary (2), Analysis (6), Profile (1)\n",
        "        \n",
        "        \n",
        "        if item is \"\":\n",
        "          df = df.transpose()\n",
        "          item_df = pd.concat([item_df, df], axis = 1, ignore_index = True)\n",
        "          \n",
        "          \n",
        "        if item == \"/analysis\": \n",
        "          cols_total = df.shape[1]\n",
        "          column_heads = []\n",
        "          for head in range(1, cols_total + 1):\n",
        "            column_heads.append(str(head))\n",
        "          \n",
        "          df.columns = column_heads\n",
        "          \n",
        "          df.set_index(df.columns[0], inplace=True)\n",
        "          df = df.replace({pd.np.nan: None})\n",
        "          df = df.unstack().to_frame().sort_index(level=1).T\n",
        "          \n",
        "          df.columns = df.columns.map(\"_\".join)\n",
        "          \n",
        "          item_df = pd.concat([item_df, df], axis = 1)\n",
        "          \n",
        "      \n",
        "        if item == \"/profile\":     \n",
        "          column_Names = df.columns\n",
        "          columns = []\n",
        "          rows = df.shape[0]\n",
        "\n",
        "          for j in range(1, rows + 1):\n",
        "            for i in column_Names:\n",
        "              columns.append(str(i) + \"_\" + str(j))\n",
        "\n",
        "          columns_series = pd.Series(columns, index  = None)\n",
        "          values_series = pd.Series(df.values.flatten(), index = None)\n",
        "          values_list = [df.values.flatten()]\n",
        "          item_df = pd.DataFrame(values_list, columns = columns)\n",
        "\n",
        "      if item is \"\":\n",
        "        new_header = item_df.iloc[0] #grab the first row for the header\n",
        "        item_df.columns = new_header #set the header row as the df header\n",
        "        item_df = item_df[1:] #take the data less the header row\n",
        "        item_df.reset_index(inplace=True, drop=True)\n",
        "      \n",
        "      \n",
        "      #print(name, len(dataframe_list), 'tables ', item_df.shape[1], 'Columns')\n",
        "      cmp_df_list.append(item_df)\n",
        "      \n",
        "  #     filename = cmp + '_' + name + '.csv'\n",
        "  #     item_df.to_csv(filename, index = False)\n",
        "  #     !cp $filename drive/My\\ Drive/Research/AB/\n",
        "    \n",
        "    \n",
        "    # Non tabular Profile \n",
        "    profile_url = urlPre + cmp + \"/profile\" + urlSuf + cmp\n",
        "    profile_data = getProfileData(profile_url)\n",
        "    cmp_df_list.append(profile_data)\n",
        "    \n",
        "    cmp_df = pd.DataFrame()\n",
        "    for df in cmp_df_list:\n",
        "      cmp_df = pd.concat([cmp_df,df], axis = 1)\n",
        "    \n",
        "    # Causes problem when company name is \"F\" and it gets replaced everywhere in the column names. Better to do in parse_url\n",
        "    # cmp_df.columns = cmp_df.columns.str.replace(cmp, \"cmp\")\n",
        "    \n",
        "    if columns == None:\n",
        "      columns = cmp_df.columns.tolist()\n",
        "  \n",
        "    #cmp_df.sort_index(axis=1, inplace=True) # not required if string replace done at parsing time\n",
        "    \n",
        "    if (final_df.shape[0] != 0 and final_df.columns.equals(cmp_df.columns) == False ):\n",
        "      # except when final_df is empty. This should return always True. Else column mistmatch (ignore this company)\n",
        "      print('Column mismatch for company: ', cmp, ' Ignoring...')\n",
        "      #print('differences: ', cmp_df.columns.difference(final_df.columns), final_df.columns.difference(cmp_df.columns)) #should always print 0\n",
        "      ignored_cmp.append(cmp)\n",
        "      continue\n",
        "      # MSFT: has current year (2020): typo\n",
        "      # ADBE: has different dates\n",
        "    else:\n",
        "      towriteCmpList.append(cmp)\n",
        "    \n",
        "    \n",
        "    \n",
        "    final_df = pd.concat([final_df, cmp_df], ignore_index = True)\n",
        "\n",
        "  final_df['company'] = towriteCmpList  \n",
        "  final_df = final_df.set_index('company')\n",
        "  print('Final Data: ',final_df.shape)\n",
        "\n",
        "  filename = 'YahooDataProfileLarge.xlsx'\n",
        "  final_df.to_excel(filename)\n",
        "  !cp $filename drive/My\\ Drive/Research/AB/\n",
        "\n",
        "  if len(ignored_cmp) > 0:\n",
        "    ignored_csv = pd.DataFrame(ignored_cmp)\n",
        "    ignored_filename = 'YahooDataProfileLarge_ignoredcmp.xlsx'\n",
        "    ignored_csv.to_excel(ignored_filename, index=\"False\")\n",
        "    !cp $ignored_filename drive/My\\ Drive/Research/AB/\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y8vXhhEhC2sA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def writeYahooAnalysis():\n",
        "  # Parsing data from yahoo finance (profile, analysis, summary)\n",
        "  print(\"\\nWriting Yahoo Analysis\")\n",
        "  towriteCmpList = []\n",
        "  ignored_cmp = []\n",
        "  #https://finance.yahoo.com/quote/%5EIXIC/components?p=%5EIXIC\n",
        "\n",
        "  items = ['/analysis']\n",
        "  urlPre = 'https://finance.yahoo.com/quote/'\n",
        "  urlSuf = '?p='\n",
        "\n",
        "  columns = None\n",
        "\n",
        "  final_df = pd.DataFrame()\n",
        "\n",
        "  for cmp in cmpList:\n",
        "    cmp_df_list = []\n",
        "    print('\\nCompany: ', cmp)\n",
        "    \n",
        "    for item in items:\n",
        "      url = urlPre + cmp + item + urlSuf + cmp\n",
        "      dataframe_list = parse_url(url, cmp)\n",
        "      \n",
        "      if item is '':\n",
        "        name = 'summary'\n",
        "      else:\n",
        "        name = item[1:len(item)]\n",
        "        \n",
        "      count = 1\n",
        "      \n",
        "      item_df = pd.DataFrame()\n",
        "      \n",
        "      \n",
        "      for df in dataframe_list:\n",
        "      \n",
        "        # Summary (2), Analysis (6), Profile (1)\n",
        "        \n",
        "        \n",
        "        if item is \"\":\n",
        "          df = df.transpose()\n",
        "          item_df = pd.concat([item_df, df], axis = 1, ignore_index = True)\n",
        "          \n",
        "          \n",
        "        if item == \"/analysis\": \n",
        "          cols_total = df.shape[1]\n",
        "          column_heads = []\n",
        "          for head in range(1, cols_total + 1):\n",
        "            column_heads.append(str(head))\n",
        "          \n",
        "          df.columns = column_heads\n",
        "          \n",
        "          df.set_index(df.columns[0], inplace=True)\n",
        "          df = df.replace({pd.np.nan: None})\n",
        "          df = df.unstack().to_frame().sort_index(level=1).T\n",
        "          \n",
        "          df.columns = df.columns.map(\"_\".join)\n",
        "          \n",
        "          item_df = pd.concat([item_df, df], axis = 1)\n",
        "          \n",
        "      \n",
        "        if item == \"/profile\":     \n",
        "          column_Names = df.columns\n",
        "          columns = []\n",
        "          rows = df.shape[0]\n",
        "\n",
        "          for j in range(1, rows + 1):\n",
        "            for i in column_Names:\n",
        "              columns.append(str(i) + \"_\" + str(j))\n",
        "\n",
        "          columns_series = pd.Series(columns, index  = None)\n",
        "          values_series = pd.Series(df.values.flatten(), index = None)\n",
        "          values_list = [df.values.flatten()]\n",
        "          item_df = pd.DataFrame(values_list, columns = columns)\n",
        "\n",
        "      if item is \"\":\n",
        "        new_header = item_df.iloc[0] #grab the first row for the header\n",
        "        item_df.columns = new_header #set the header row as the df header\n",
        "        item_df = item_df[1:] #take the data less the header row\n",
        "        item_df.reset_index(inplace=True, drop=True)\n",
        "      \n",
        "      \n",
        "      #print(name, len(dataframe_list), 'tables ', item_df.shape[1], 'Columns')\n",
        "      cmp_df_list.append(item_df)\n",
        "      \n",
        "  #     filename = cmp + '_' + name + '.csv'\n",
        "  #     item_df.to_csv(filename, index = False)\n",
        "  #     !cp $filename drive/My\\ Drive/Research/AB/\n",
        "    \n",
        "    \n",
        "    # Non tabular Profile \n",
        "    profile_url = urlPre + cmp + \"/profile\" + urlSuf + cmp\n",
        "    profile_data = getProfileData(profile_url)\n",
        "    cmp_df_list.append(profile_data)\n",
        "    \n",
        "    cmp_df = pd.DataFrame()\n",
        "    for df in cmp_df_list:\n",
        "      cmp_df = pd.concat([cmp_df,df], axis = 1)\n",
        "    \n",
        "    # Causes problem when company name is \"F\" and it gets replaced everywhere in the column names. Better to do in parse_url\n",
        "    # cmp_df.columns = cmp_df.columns.str.replace(cmp, \"cmp\")\n",
        "    \n",
        "    if columns == None:\n",
        "      columns = cmp_df.columns.tolist()\n",
        "  \n",
        "    #cmp_df.sort_index(axis=1, inplace=True) # not required if string replace done at parsing time\n",
        "    \n",
        "    if (final_df.shape[0] != 0 and final_df.columns.equals(cmp_df.columns) == False ):\n",
        "      # except when final_df is empty. This should return always True. Else column mistmatch (ignore this company)\n",
        "      print('Column mismatch for company: ', cmp, ' Ignoring...')\n",
        "      #print('differences: ', cmp_df.columns.difference(final_df.columns), final_df.columns.difference(cmp_df.columns)) #should always print 0\n",
        "      ignored_cmp.append(cmp)\n",
        "      continue\n",
        "      # MSFT: has current year (2020): typo\n",
        "      # ADBE: has different dates\n",
        "    else:\n",
        "      towriteCmpList.append(cmp)\n",
        "    \n",
        "    \n",
        "    \n",
        "    final_df = pd.concat([final_df, cmp_df], ignore_index = True)\n",
        "\n",
        "  final_df['company'] = towriteCmpList  \n",
        "  final_df = final_df.set_index('company')\n",
        "  print('Final Data: ',final_df.shape)\n",
        "\n",
        "  filename = 'YahooDataAnalysisLarge.xlsx'\n",
        "  final_df.to_excel(filename)\n",
        "  !cp $filename drive/My\\ Drive/Research/AB/\n",
        "\n",
        "  if len(ignored_cmp) > 0:\n",
        "    ignored_csv = pd.DataFrame(ignored_cmp)\n",
        "    ignored_filename = 'YahooDataAnalysisLarge_ignoredcmp.xlsx'\n",
        "    ignored_csv.to_excel(ignored_filename, index=\"False\")\n",
        "    !cp $ignored_filename drive/My\\ Drive/Research/AB/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88DDkNh_LDXD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "  # Parsing data from marketwatch (financials)\n",
        "def writeMarketWatch():\n",
        "  print(\"\\nWriting Market Watch Financials\")\n",
        "  towriteCmpList = []\n",
        "  ignored_cmp = []\n",
        "  #https://finance.yahoo.com/quote/%5EIXIC/components?p=%5EIXIC\n",
        "\n",
        "  items = ['/financials', '/financials/balance-sheet', '/financials/cash-flow']\n",
        "  # items = ['/financials']\n",
        "  urlPre = 'https://www.marketwatch.com/investing/stock/'\n",
        "\n",
        "  columns = None\n",
        "\n",
        "\n",
        "  final_df = pd.DataFrame()\n",
        "\n",
        "  for cmp in cmpList:\n",
        "    cmp_df_list = []\n",
        "    print('\\nCompany: ', cmp)\n",
        "    \n",
        "    for item in items:\n",
        "      url = urlPre + cmp + item\n",
        "      dataframe_list = parse_url(url, cmp)\n",
        "      \n",
        "     \n",
        "      name = item[1:len(item)]\n",
        "    \n",
        "      count = 1\n",
        "      \n",
        "      item_df = pd.DataFrame()\n",
        "      \n",
        "      for df in dataframe_list:\n",
        "        cols_total = df.shape[1]\n",
        "        column_heads = []\n",
        "        for head in range(cols_total):\n",
        "          column_heads.append(str(head))\n",
        "        \n",
        "        df.columns = column_heads\n",
        "        df.set_index(df.columns[0], inplace=True)\n",
        "        df = df.replace({pd.np.nan: None})\n",
        "        df = df.unstack().to_frame().sort_index(level=1).T\n",
        "        df.columns = df.columns.map(\"_\".join)\n",
        "        item_df = pd.concat([item_df, df], axis = 1)\n",
        "      \n",
        "    \n",
        "      # print(item_df)\n",
        "      # print(name, len(dataframe_list), 'tables ', item_df.shape[1], 'Columns')\n",
        "      cmp_df_list.append(item_df)\n",
        "      \n",
        "  #     filename = cmp + '_' + name + '.csv'\n",
        "  #     item_df.to_csv(filename, index = False)\n",
        "  #     !cp $filename drive/My\\ Drive/Research/AB/\n",
        "    \n",
        "    \n",
        "    \n",
        "    cmp_df = pd.DataFrame()\n",
        "    for df in cmp_df_list:\n",
        "      cmp_df = pd.concat([cmp_df,df], axis = 1)\n",
        "    \n",
        "    # Causes problem when company name is \"F\" and it gets replaced everywhere in the column names. Better to do in parse_url\n",
        "    # cmp_df.columns = cmp_df.columns.str.replace(cmp, \"cmp\")\n",
        "    \n",
        "    if columns == None:\n",
        "      columns = cmp_df.columns.tolist()\n",
        "  \n",
        "    #cmp_df.sort_index(axis=1, inplace=True) # not required if string replace done at parsing time\n",
        "    \n",
        "    if (final_df.shape[0] != 0 and final_df.columns.equals(cmp_df.columns) == False ):\n",
        "      # except when final_df is empty. This should return always True. Else column mistmatch (ignore this company)\n",
        "      print('Column mismatch for company: ', cmp, ' Ignoring...')\n",
        "      #print('differences: ', cmp_df.columns.difference(final_df.columns), final_df.columns.difference(cmp_df.columns)) #should always print 0\n",
        "      ignored_cmp.append(cmp)\n",
        "      continue\n",
        "      # MSFT: has current year (2020): typo\n",
        "      # ADBE: has different dates\n",
        "      # column name different issue fixed. Now different row names causing error\n",
        "    else:\n",
        "      towriteCmpList.append(cmp)\n",
        "    \n",
        "    \n",
        "    \n",
        "    final_df = pd.concat([final_df, cmp_df], ignore_index = True)\n",
        "\n",
        "  final_df['company'] = towriteCmpList  \n",
        "  final_df = final_df.set_index('company')\n",
        "  print('Final Data: ',final_df.shape)\n",
        "\n",
        "  filename = 'MarketWatchDataLarge.xlsx'\n",
        "  final_df.to_excel(filename)\n",
        "  !cp $filename drive/My\\ Drive/Research/AB/\n",
        "\n",
        "  if len(ignored_cmp) > 0:\n",
        "    ignored_csv = pd.DataFrame(ignored_cmp)\n",
        "    ignored_filename = 'MarketWatchDataLarge_ignoredcmp.xlsx'\n",
        "    ignored_csv.to_excel(ignored_filename, index=\"False\")\n",
        "    !cp $ignored_filename drive/My\\ Drive/Research/AB/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qEAUtvOuU334",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " def writeYahooFinancial(): \n",
        "  # Parsing data from yahoo (financials)\n",
        "  towriteCmpList = []\n",
        "  ignored_cmp = []\n",
        "  finalcolsize = 376\n",
        "\n",
        "  items = ['/financials?p=', '/balance-sheet?p=', '/cash-flow?p=']\n",
        "  urlPre = 'https://finance.yahoo.com/quote/'\n",
        "\n",
        "  columns = None\n",
        "\n",
        "\n",
        "  final_df = pd.DataFrame()\n",
        "\n",
        "  for cmp in cmpList:\n",
        "    cmp_df_list = []\n",
        "    print('\\nCompany: ', cmp)\n",
        "    \n",
        "    for item in items:\n",
        "      url = urlPre + cmp + item + cmp\n",
        "      dataframe_list = parse_url(url, cmp)\n",
        "      \n",
        "      \n",
        "      name = item[1:len(item) - 3]\n",
        "      \n",
        "      count = 1\n",
        "      \n",
        "      item_df = getFinanceData(url, name)\n",
        "      \n",
        "    \n",
        "      print(name, len(item_df), 'table ', item_df.shape[1], 'Columns')\n",
        "      cmp_df_list.append(item_df)\n",
        "      \n",
        "  #     filename = cmp + '_' + name + '.csv'\n",
        "  #     item_df.to_csv(filename, index = False)\n",
        "  #     !cp $filename drive/My\\ Drive/Research/AB/\n",
        "    \n",
        "    \n",
        "  \n",
        "    cmp_df = pd.DataFrame()\n",
        "    for df in cmp_df_list:\n",
        "      cmp_df = pd.concat([cmp_df,df], axis = 1)\n",
        "    \n",
        "    # Causes problem when company name is \"F\" and it gets replaced everywhere in the column names. Better to do in parse_url\n",
        "    # cmp_df.columns = cmp_df.columns.str.replace(cmp, \"cmp\")\n",
        "    \n",
        "    if columns == None:\n",
        "      columns = cmp_df.columns.tolist()\n",
        "  \n",
        "    #cmp_df.sort_index(axis=1, inplace=True) # not required if string replace done at parsing time\n",
        "    \n",
        "    if final_df.shape[0] == 0:\n",
        "       if cmp_df.shape[1] != finalcolsize:\n",
        "        # if first cmp, size of columns should match else exact columns match should return always True. Else column mistmatch (ignore this company)\n",
        "        print('Column mismatch for company: ', cmp, ' Ignoring...')\n",
        "        #print('differences: ', cmp_df.columns.difference(final_df.columns), final_df.columns.difference(cmp_df.columns)) #should always print 0\n",
        "        ignored_cmp.append(cmp)\n",
        "        continue\n",
        "      # APPL, TSLA number of columns in balance sheets are different\n",
        "    elif final_df.columns.equals(cmp_df.columns) == False : \n",
        "      print('Column mismatch for company: ', cmp, ' Ignoring...')\n",
        "      #print('differences: ', cmp_df.columns.difference(final_df.columns), final_df.columns.difference(cmp_df.columns)) #should always print 0\n",
        "      ignored_cmp.append(cmp)\n",
        "      continue\n",
        "    else:\n",
        "      towriteCmpList.append(cmp)\n",
        "    \n",
        "    \n",
        "    \n",
        "    final_df = pd.concat([final_df, cmp_df], ignore_index = True)\n",
        "\n",
        "\n",
        "  final_df['company'] = towriteCmpList  \n",
        "  final_df = final_df.set_index('company')\n",
        "  print('Final Data: ',final_df.shape)\n",
        "\n",
        "  filename = 'YahooFinancesLarge.xlsx'\n",
        "  final_df.to_excel(filename)\n",
        "  !cp $filename drive/My\\ Drive/Research/AB/\n",
        "\n",
        "  if len(ignored_cmp) > 0:\n",
        "    ignored_csv = pd.DataFrame(ignored_cmp)\n",
        "    ignored_filename = 'YahooFinancesLarge_ignoredcmp.xlsx'\n",
        "    ignored_csv.to_excel(ignored_filename, index=\"False\")\n",
        "    !cp $ignored_filename drive/My\\ Drive/Research/AB/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-C5KK5A4oeb",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TW1JW1pozJkf",
        "colab_type": "code",
        "outputId": "49395e00-967d-48da-d5c3-8db20b34b18d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "writeYahooFinancial()\n",
        "# writeYahooAnalysis()\n",
        "# writeYahooSummary()\n",
        "# writeYahooProfile()\n",
        "# writeMarketWatch()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Company:  A\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
            "  InsecureRequestWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "financials 1 table  95 Columns\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
            "  InsecureRequestWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "balance-sheet 1 table  136 Columns\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
            "  InsecureRequestWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "cash-flow 1 table  145 Columns\n",
            "\n",
            "Company:  AA\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
            "  InsecureRequestWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "financials 1 table  95 Columns\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
            "  InsecureRequestWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "balance-sheet 1 table  136 Columns\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
            "  InsecureRequestWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "cash-flow 1 table  145 Columns\n",
            "\n",
            "Company:  AAL\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
            "  InsecureRequestWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "financials 1 table  95 Columns\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
            "  InsecureRequestWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "balance-sheet 1 table  136 Columns\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
            "  InsecureRequestWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "cash-flow 1 table  145 Columns\n",
            "\n",
            "Company:  AAPL\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
            "  InsecureRequestWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "financials 1 table  95 Columns\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
            "  InsecureRequestWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "balance-sheet 1 table  136 Columns\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
            "  InsecureRequestWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "cash-flow 1 table  145 Columns\n",
            "\n",
            "Company:  ABEV\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
            "  InsecureRequestWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "financials 1 table  95 Columns\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
            "  InsecureRequestWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "balance-sheet 1 table  136 Columns\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
            "  InsecureRequestWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "cash-flow 1 table  145 Columns\n",
            "\n",
            "Company:  ABG\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
            "  InsecureRequestWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "financials 1 table  95 Columns\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
            "  InsecureRequestWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "balance-sheet 1 table  136 Columns\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
            "  InsecureRequestWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "cash-flow 1 table  145 Columns\n",
            "\n",
            "Company:  ABIO\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
            "  InsecureRequestWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "financials 1 table  95 Columns\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
            "  InsecureRequestWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "balance-sheet 0 table  0 Columns\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
            "  InsecureRequestWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "cash-flow 1 table  145 Columns\n",
            "Column mismatch for company:  ABIO  Ignoring...\n",
            "\n",
            "Company:  ACH\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j7qlWtvv11q7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "import gspread_dataframe as gd\n",
        "\n",
        "# Connecting with `gspread` here\n",
        "\n",
        "ws = gc.open(\"SheetName\").worksheet(\"xyz\")\n",
        "existing = gd.get_as_dataframe(ws)\n",
        "updated = existing.append(your_new_data)\n",
        "gd.set_with_dataframe(ws, updated)\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N43dmWdYQVzf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}